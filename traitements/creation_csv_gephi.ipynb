{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc6f147",
   "metadata": {},
   "source": [
    "## CSV pour GEPHI :\n",
    "- ~~utiliser co-occu mots mais aussi juste mots/hashtags/mentions pr√©sents dans un m√™me tweet~~\n",
    "- ~~plusieurs graphes : un avec tous les types, un avec seulement les personnes, un avec seulement #, un @~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4222be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fanny/anaconda3/envs/main/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import json,glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def ouvrir_json(chemin):\n",
    "    with open(chemin, encoding=\"utf-8\") as f:\n",
    "        toto = json.load(f)\n",
    "    return toto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5405695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [2:47:44<00:00, 122.74s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"V2 AVEC LEMMATISATION ET FILTRE : bcp + long\"\"\"\n",
    "version = \"lem_filtree\"\n",
    "d = datetime.now()\n",
    "date = str(d.day)+\"-0\"+str(d.month)\n",
    "filtre_min = 50 #nb occu min qu'un noeud doit avoir pour √™tre mis dans le CSV\n",
    "    \n",
    "\"\"\"Cr√©er les structures de donn√©es pour remplir ensuite les CSV : un fichier de noeuds avec id, label, type, weight (nb_occu)\n",
    "et un fichier de relations avec source, target, weight\"\"\"\n",
    "\n",
    "co_o = []\n",
    "id_mot = {}\n",
    "k = 0\n",
    "n = []\n",
    "\n",
    "for doc in tqdm.tqdm(glob.glob(\"corpus_projet_plus/*\")):\n",
    "    corpus = ouvrir_json(doc)\n",
    "    data = corpus[\"data\"]\n",
    "    for n_tweet, tweet in enumerate(data):\n",
    "        text = tweet[\"text\"].lower()\n",
    "\n",
    "        text_tok = text.lower().split()\n",
    "        #text_tok = [tok for tok in text_tok if (tok.isalpha() or tok.startswith(\"@\") or tok.startswith('#'))] #and tok not in \".,;!?üëâ\"] #and tok.isalpha() mais perd @ et #\n",
    "        text_tok = [\"hashtag\"+tok if tok.startswith('#') else tok for tok in text_tok] #astuce sinon lemmatisation enl√®ve les di√®ses donc on perd notion = √† enlever plus tard avec regex de hashtag\n",
    "        text_lem = nlp(\" \".join(text_tok))\n",
    "        lem = [str(tok.lemma_) for tok in text_lem if tok.lemma_ not in fr_stop and (tok.lemma_.isalpha() or tok.lemma_.startswith(\"@\") or tok.lemma_.startswith('hashtag#'))]\n",
    "        #filtrer mots vides (ici, pas avant, sinon peut biaiser lemmatisation) + ponctuation\n",
    "\n",
    "        \n",
    "        \"\"\"Pour le fichier de noeuds : id (nb incr√©ment√©), type mot, nb_occu (poids)\"\"\"\n",
    "        for token in lem:\n",
    "            #toutes les lignes qui sont de cette forme servent √† regrouper les hashtags type #reformedesretraites (avec/sans s, accent, etc)\n",
    "            if (\"reforme\" in token or \"r√©forme\" in token) and \"retraite\" in token:\n",
    "                token = \"hashtag#reformedesretraites\"\n",
    "            if token not in id_mot:\n",
    "                nb_occu = 1\n",
    "                type_mot = \"mot\" \n",
    "                if token.startswith(\"@\"):\n",
    "                    type_mot = \"personne\"\n",
    "                if token.startswith(\"hashtag#\"):\n",
    "                    type_mot = \"hashtag\"\n",
    "                id_mot[token] = [k, type_mot, nb_occu]\n",
    "                k+=1\n",
    "            else:\n",
    "                id_mot[token][2]+=1\n",
    "        \n",
    "        \"\"\"Pour le fichier de relations : toutes les combinaisons possibles dans le tweet (lier tous mots d'un m√™me tweet)\n",
    "        mais compter qu'une seule fois la relation par tweet\"\"\"       \n",
    "        for i in range(len(lem)):\n",
    "            mot = lem[i]\n",
    "            if (\"reforme\" in mot or \"r√©forme\" in mot) and \"retraite\" in mot:\n",
    "                mot = \"hashtag#reformedesretraites\"\n",
    "            if i < len(lem)-1 and mot not in fr_stop and (mot.isalpha() or mot.startswith(\"@\") or mot.startswith('#') or mot.startswith('hashtag#')):\n",
    "                for j in range(len(lem)+1):\n",
    "                    if j < len(lem)-2:\n",
    "                        if j == i:\n",
    "                            j+=1\n",
    "                        if (\"reforme\" in lem[j] or \"r√©forme\" in lem[j]) and \"retraite\" in lem[j]:\n",
    "                            mot_svt = \"hashtag#reformedesretraites\"\n",
    "                        else:\n",
    "                            mot_svt = lem[j]\n",
    "                        co_o.append(sorted([mot,mot_svt]))\n",
    "                        #if sorted([mot,mot_svt]) not in co_o or n_tweet not in n:\n",
    "                            #co_o.append(sorted([mot,mot_svt]))\n",
    "                            #n.append(n_tweet) #pour v√©rifier\n",
    "    \n",
    "\"\"\"Pour ne pas avoir √† relancer toute la lemmatisation etc\"\"\"\n",
    "\n",
    "with open('id_mot.json', 'w') as f:\n",
    "    json.dump(id_mot, f, indent=4)\n",
    "with open('co_o.json', 'w') as f:\n",
    "    json.dump(co_o, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b91c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclus = []   \n",
    "\"\"\"√âcrire le fichier de noeuds\"\"\"\n",
    "with open(f'csv/noeuds_{date}_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\",\"Label\",\"Kind\",\"Weight\"])\n",
    "    for mot, liste_att in id_mot.items(): #dico forme {mot : [id, type, nb_occu]}\n",
    "        if liste_att[1] == \"mot\": #filtre + strict sur les mots\n",
    "            filtre=150\n",
    "        else:\n",
    "            filtre= filtre_min\n",
    "        if liste_att[2] > filtre:\n",
    "            if mot.startswith(\"hashtag#\"): #r√©gler le trick utilis√© pour contourner la lemmatisation des hashtags\n",
    "                mot = mot.split(\"hashtag\")[-1]\n",
    "            writer.writerow([liste_att[0],mot,liste_att[1],liste_att[2]])\n",
    "        else: #mettre de c√¥t√© les id de noeuds filtr√©s pour pas les prendre dans les relations\n",
    "            exclus.append(liste_att[0])\n",
    "            \n",
    "\"\"\"Remplacer les mots par leur id pour le fichier de relations et compter le nombre de co-occurrences pour poids\"\"\"\n",
    "from collections import Counter\n",
    "counter = Counter(tuple(x) for x in co_o)\n",
    "id_o = [[id_mot[k[0]][0], id_mot[k[1]][0], v] for k,v in counter.items()]\n",
    "#id_o[:10]   \n",
    "\n",
    "with open(f'csv/liens_{date}_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\",\"Target\",\"Weight\"])\n",
    "    for liste in id_o: #liste de listes type [source, target, weight]\n",
    "        if liste[0] not in exclus and liste[1] not in exclus:\n",
    "            writer.writerow([liste[0], liste[1],liste[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e42ff8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filtrer fichiers de noeuds et relations pour faire d'autres graphiques zoom√©s : un pour les personnes,\n",
    "un pour les hashtags, un pour les mots\"\"\"\n",
    "personne = []\n",
    "id_p = []\n",
    "hashtag = []\n",
    "id_h = []\n",
    "mot = []\n",
    "id_m = []\n",
    "\n",
    "with open(f'../csv/noeuds_{date}_lem_filtree.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i,row in enumerate(reader):\n",
    "        if i>0:\n",
    "            if row[2]==\"personne\":\n",
    "                personne.append(row) #pour pouvoir remplir le csv noeuds personne ensuite\n",
    "                id_p.append(row[0]) #pour pouvoir remplir le csv liens personne ensuite (avec seulement noeuds de source/sortie inclus)\n",
    "            if row[2]==\"hashtag\":\n",
    "                hashtag.append(row)\n",
    "                id_h.append(row[0])\n",
    "            if row[2]==\"mot\":\n",
    "                mot.append(row)\n",
    "                id_m.append(row[0])\n",
    "\n",
    "#liste de listes contenant les lignes √† garder pour les nvx fichiers de relations\n",
    "liens_p = []\n",
    "liens_h = []\n",
    "liens_m = []\n",
    "with open(f'../csv/liens_{date}_lem_filtree.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i>0:\n",
    "            if row[0] in id_m and row[1] in id_m:\n",
    "                liens_m.append(row)\n",
    "            if row[0] in id_h and row[1] in id_h:\n",
    "                liens_h.append(row)\n",
    "            if row[0] in id_p and row[1] in id_p:\n",
    "                liens_p.append(row)\n",
    "                \n",
    "#√©crire nvx csv\n",
    "kind = [\"mots\",\"personnes\",\"hashtags\"]\n",
    "liens_kind = [liens_m,liens_p,liens_h]\n",
    "noeuds_kind = [mot,personne,hashtag]\n",
    "for i, k in enumerate(kind):\n",
    "    with open(f'../csv/liens_{date}_{k}.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Source\",\"Target\",\"Weight\"])\n",
    "        for row in liens_kind[i]:\n",
    "            writer.writerow(row)\n",
    "    with open(f'../csv/noeuds_{date}_{k}.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Id\",\"Label\",\"Kind\",\"Weight\"])\n",
    "        for row in noeuds_kind[i]:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5518ea7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:00<00:00, 257.58it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Seulement sur users √† + de 100k followers, pour mentions = graphe avec direction !!!\"\"\"\n",
    "version = \"populaire\"\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "import csv\n",
    "d = datetime.now()\n",
    "date = str(d.day)+\"-0\"+str(d.month)\n",
    "filtre_min = 1 #nb occu min qu'un noeud doit avoir pour √™tre mis dans le CSV\n",
    "\n",
    "k = 0\n",
    "popular = {} #dico type {user:[id]} pour pouvoir ensuite seulement garder tweets de ces gens\n",
    "pop_mh = {} #pour garder les id des mentions et hashtags utilis√©s par un utilisateur\n",
    "poids = {} #pour donner poids aux noeuds selon nb followers (pour users)\n",
    "id_mot = {}\n",
    "\n",
    "for doc in tqdm.tqdm(glob.glob(\"../corpus/*\")):\n",
    "    corpus = ouvrir_json(doc)\n",
    "    data = corpus[\"data\"]\n",
    "    includes = corpus[\"includes\"]\n",
    "    \n",
    "    \"\"\"PARTIE INCLUDES/USERS\"\"\"\n",
    "    for user in includes[\"users\"]:\n",
    "        if user['public_metrics']['followers_count'] > 100000:\n",
    "            popular[user['id']]=user['username']\n",
    "            poids[user['id']]= user['public_metrics']['followers_count']\n",
    "                \n",
    "    for n_tweet, tweet in enumerate(data):\n",
    "        if tweet[\"author_id\"] in popular.keys():\n",
    "            text = tweet[\"text\"]\n",
    "            text = text.replace(\",.-;:?!)(\",\"\")\n",
    "            text = text.replace(\",\",\"\")\n",
    "\n",
    "            text_tok = text.split()\n",
    "            \n",
    "            #garder seulement mentions (et hashtags)\n",
    "            text_tok = [tok for tok in text_tok if tok.startswith(\"@\")] #or tok.startswith('#')] #and tok not in \".,;!?üëâ\"] #and tok.isalpha() mais perd @ et #\n",
    "\n",
    "            \"\"\"Pour le fichier de noeuds : id (nb incr√©ment√©), type mot, nb_occu (poids)\"\"\"\n",
    "            for token in text_tok:\n",
    "                if token not in id_mot:\n",
    "                    nb_occu = 1\n",
    "                    id_mot[token] = [k, type_mot, nb_occu]\n",
    "                    k+=1\n",
    "                else:\n",
    "                    id_mot[token][2]+=1\n",
    "\n",
    "                #ne pas cr√©er de nv identifiant si la personne mentionn√©e fait partie des users s√©lectionn√©s\n",
    "                if token.split('@')[-1] in popular.values():\n",
    "                    key = list(filter(lambda x: popular[x] == token.split('@')[-1], popular))[0]\n",
    "                if tweet['author_id'] not in pop_mh:\n",
    "                    #dico de dico {userid : {id_mention : nb_occu_mention}}\n",
    "                    pop_mh[tweet['author_id']] = {}\n",
    "                if token.split('@')[-1] in popular.values():\n",
    "                    pop_mh[tweet['author_id']][int(key)] = pop_mh[tweet['author_id']].get(int(key),0)+1\n",
    "                else:\n",
    "                    pop_mh[tweet['author_id']][k] = pop_mh[tweet['author_id']].get(k,0)+1\n",
    "                    \n",
    "                    \n",
    "#√âcrire le fichier de noeuds en ajoutant les utilisateurs aussi\n",
    "with open(f'../csv/noeuds_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\",\"Label\",\"Kind\",\"Weight\"])\n",
    "    for mot, liste_att in id_mot.items(): #dico forme {mot : [id, type, nb_occu]}\n",
    "        writer.writerow([liste_att[0],mot,liste_att[1],liste_att[2]])\n",
    "    for idd, username in popular.items():#ajouter les utilisateurs populaires (comme personne mais avec poids √† 100)\n",
    "        writer.writerow([idd, \"@\"+username, \"personne\", round(poids[idd]/10000)])\n",
    "            \n",
    "\n",
    "with open(f'../csv/liens_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\",\"Target\",\"Type\",\"Weight\"])\n",
    "    for userid, liste_m in pop_mh.items(): \n",
    "        for m,nb in liste_m.items():\n",
    "            writer.writerow([userid, m, \"Directed\", nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6be7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [06:30<00:00, 16.99s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"OBSOL√àTE\n",
    "V1 SANS LEMMATISATION NI FILTRE\"\"\"\n",
    "\"\"\"Cr√©er les structures de donn√©es pour remplir ensuite les CSV : un fichier de noeuds avec id, label, type, weight (nb_occu)\n",
    "et un fichier de relations avec source, target, weight\"\"\"\n",
    "\n",
    "version = \"filtre\"\n",
    "co_o = []\n",
    "id_mot = {}\n",
    "k = 0\n",
    "n = []\n",
    "\n",
    "for doc in tqdm.tqdm(glob.glob(\"corpus_projet_plus/*\")):\n",
    "    corpus = ouvrir_json(doc)\n",
    "    data = corpus[\"data\"]\n",
    "    for n_tweet, tweet in enumerate(data):\n",
    "        text = tweet[\"text\"].lower()\n",
    "\n",
    "        text_tok = text.lower().split()\n",
    "        text_tok = [tok for tok in text_tok if tok not in fr_stop and (tok.isalpha() or tok.startswith(\"@\") or tok.startswith('#'))] #and tok not in \".,;!?üëâ\"] #and tok.isalpha() mais perd @ et #\n",
    "\n",
    "        \"\"\"Pour le fichier de noeuds : id (nb incr√©ment√©), type mot, nb_occu (poids)\"\"\"\n",
    "        for token in text_tok:\n",
    "            if token not in id_mot:\n",
    "                nb_occu = 1\n",
    "                type_mot = \"mot\" \n",
    "                if token.startswith(\"@\"):\n",
    "                    type_mot = \"personne\"\n",
    "                if token.startswith(\"#\"):\n",
    "                    type_mot = \"hashtag\"\n",
    "                id_mot[token] = [k, type_mot, nb_occu]\n",
    "                k+=1\n",
    "            else:\n",
    "                id_mot[token][2]+=1\n",
    "        \n",
    "        \"\"\"Pour le fichier de relations : toutes les combinaisons possibles dans le tweet (lier tous mots d'un m√™me tweet)\n",
    "        mais compter qu'une seule fois la relation par tweet\"\"\"       \n",
    "        for i in range(len(text_tok)):\n",
    "            mot = text_tok[i]\n",
    "            if i < len(text_tok)-1 and mot not in fr_stop and (mot.isalpha() or mot.startswith(\"@\") or mot.startswith('#')):\n",
    "                for j in range(len(text_tok)+1):\n",
    "                    if j < len(text_tok)-2:\n",
    "                        if j == i:\n",
    "                            j+=1\n",
    "                        if sorted([mot,text_tok[j]]) not in co_o or n_tweet not in n:\n",
    "                            co_o.append(sorted([mot,text_tok[j]]))\n",
    "                            n.append(n_tweet) #pour v√©rifier\n",
    "\n",
    "\"\"\"√âcrire le fichier de noeuds\"\"\"\n",
    "exclus = []\n",
    "with open(f'csv/noeuds_2301_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\",\"Label\",\"Kind\",\"Weight\"])\n",
    "    for mot, liste_att in id_mot.items(): #dico forme {mot : [id, type, nb_occu]}\n",
    "        if liste_att[2] > 10:\n",
    "            writer.writerow([liste_att[0],mot,liste_att[1],liste_att[2]])\n",
    "        else: #mettre de c√¥t√© les id de noeuds pas sauvegard√©s pour pas les prendre dans les relations\n",
    "            exclus.append(liste_att[0])\n",
    "        \n",
    "\"\"\"Remplacer les mots par leur id pour le fichier de relations\"\"\"\n",
    "id_o = []\n",
    "for k in co_o: #pour les poids\n",
    "    ligne = []\n",
    "    for mot in k:\n",
    "        ligne.append(id_mot[mot][0])\n",
    "    ligne.append(co_o.count(k))\n",
    "    id_o.append(ligne)\n",
    "\n",
    "with open(f'csv/liens_2301_{version}.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\",\"Target\",\"Weight\"])\n",
    "    for liste in id_o: #liste de listes type [source, target, weight]\n",
    "         if liste[0] not in exclus and liste[1] not in exclus:\n",
    "            writer.writerow([liste[0], liste[1],liste[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
